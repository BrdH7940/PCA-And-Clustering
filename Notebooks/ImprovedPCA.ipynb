{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, silhouette_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. READING THE CSV FILE\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load data from CSV and perform initial exploration\n",
    "    \"\"\"\n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Basic information about the dataset\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Number of samples: {df.shape[0]}\")\n",
    "    print(f\"Number of features: {df.shape[1]}\")\n",
    "\n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum().sum()\n",
    "    print(f\"Missing values: {missing_values}\")\n",
    "\n",
    "    # Extract class distribution\n",
    "    if 'group' in df.columns:\n",
    "        print(\"Class distribution:\")\n",
    "        print(df['group'].value_counts())\n",
    "\n",
    "    return df\n",
    "\n",
    "# 2. FEATURE TRANSFORMATIONS\n",
    "def transform_features(X):\n",
    "    \"\"\"\n",
    "    Apply feature transformations to improve separability\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    X_transformed = X.copy()\n",
    "\n",
    "    # 1. Log transform for highly skewed features\n",
    "    # First find highly skewed columns (absolute skew > 1)\n",
    "    skewed_features = []\n",
    "    for col in X_transformed.columns:\n",
    "        if abs(X_transformed[col].skew()) > 1:\n",
    "            skewed_features.append(col)\n",
    "\n",
    "    # Apply log transform to skewed features (adding small constant to handle zeros)\n",
    "    for col in skewed_features:\n",
    "        if (X_transformed[col] >= 0).all():  # Only apply to non-negative columns\n",
    "            X_transformed[col] = np.log1p(X_transformed[col])\n",
    "\n",
    "    print(f\"Applied log transform to {len(skewed_features)} skewed features\")\n",
    "\n",
    "    # 2. Feature interactions (for selected most important features)\n",
    "    # Since we don't know which features are most important yet,\n",
    "    # we'll create interactions between features with high variance\n",
    "\n",
    "    # Get top 10 features with highest variance\n",
    "    variances = X_transformed.var().sort_values(ascending=False)\n",
    "    top_features = variances.index[:10]\n",
    "\n",
    "    # Create pairwise interactions between top features\n",
    "    for i, feat1 in enumerate(top_features):\n",
    "        for feat2 in top_features[i+1:]:\n",
    "            interaction_name = f\"{feat1}_x_{feat2}\"\n",
    "            X_transformed[interaction_name] = X_transformed[feat1] * X_transformed[feat2]\n",
    "\n",
    "    print(f\"Added {len(top_features) * (len(top_features) - 1) // 2} interaction features\")\n",
    "\n",
    "    # 3. Polynomial features for top features (squared terms)\n",
    "    for feat in top_features:\n",
    "        X_transformed[f\"{feat}_squared\"] = X_transformed[feat] ** 2\n",
    "\n",
    "    print(f\"Added {len(top_features)} polynomial features\")\n",
    "\n",
    "    # 4. Remove features with very low variance\n",
    "    # Calculate variance for each feature\n",
    "    variances = X_transformed.var()\n",
    "    # Remove features with variance close to 0\n",
    "    low_var_threshold = 1e-5\n",
    "    low_var_features = variances[variances < low_var_threshold].index\n",
    "    X_transformed = X_transformed.drop(columns=low_var_features)\n",
    "\n",
    "    print(f\"Removed {len(low_var_features)} features with near-zero variance\")\n",
    "\n",
    "    # 5. Detect and handle outliers\n",
    "    # Using z-score method: replace extreme values (|z| > 3) with winsorized values\n",
    "    for col in X_transformed.columns:\n",
    "        z_scores = (X_transformed[col] - X_transformed[col].mean()) / X_transformed[col].std()\n",
    "        outliers = (abs(z_scores) > 3)\n",
    "        if outliers.sum() > 0:\n",
    "            # Replace outliers with threshold values (winsorizing)\n",
    "            upper_limit = X_transformed[col].mean() + 3 * X_transformed[col].std()\n",
    "            lower_limit = X_transformed[col].mean() - 3 * X_transformed[col].std()\n",
    "            X_transformed.loc[z_scores > 3, col] = upper_limit\n",
    "            X_transformed.loc[z_scores < -3, col] = lower_limit\n",
    "\n",
    "    print(\"Handled outliers using winsorization\")\n",
    "\n",
    "    return X_transformed\n",
    "\n",
    "# 3. MY PCA IMPLEMENTATION\n",
    "class MyPCA:\n",
    "    def __init__(self, n_components):\n",
    "        \"\"\"\n",
    "        Initialize PCA with number of components\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.components = None\n",
    "        self.mean = None\n",
    "        self.explained_variance = None\n",
    "        self.explained_variance_ratio = None\n",
    "        self.cumulative_explained_variance_ratio = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit PCA model to data X\n",
    "        \"\"\"\n",
    "        # Center the data (subtract mean)\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        X_centered = X - self.mean\n",
    "\n",
    "        # Compute covariance matrix\n",
    "        cov_matrix = np.cov(X_centered, rowvar=False)\n",
    "\n",
    "        # Compute eigenvalues and eigenvectors\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "        # Sort eigenvalues and eigenvectors in descending order\n",
    "        idx = np.argsort(eigenvalues)[::-1]\n",
    "        eigenvalues = eigenvalues[idx]\n",
    "        eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "        # Store the top n_components eigenvectors\n",
    "        self.components = eigenvectors[:, :self.n_components]\n",
    "\n",
    "        # Calculate explained variance and ratios\n",
    "        total_var = np.sum(eigenvalues)\n",
    "        self.explained_variance = eigenvalues[:self.n_components]\n",
    "        self.explained_variance_ratio = self.explained_variance / total_var\n",
    "        self.cumulative_explained_variance_ratio = np.cumsum(self.explained_variance_ratio)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform X using the PCA components\n",
    "        \"\"\"\n",
    "        # Center the data using saved mean\n",
    "        X_centered = X - self.mean\n",
    "\n",
    "        # Project data onto principal components\n",
    "        X_transformed = np.dot(X_centered, self.components)\n",
    "\n",
    "        return X_transformed\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        Fit to data then transform it\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "# 4. MAIN EXECUTION FLOW\n",
    "def main(file_path, test_n_components=None):\n",
    "    \"\"\"\n",
    "    Main function to execute the workflow\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    df = load_data(file_path)\n",
    "\n",
    "    # Separate features and target\n",
    "    y = df['group'].copy()\n",
    "    X = df.drop('group', axis=1)\n",
    "    X = X.select_dtypes(include=[np.number])  # Keep only numerical features\n",
    "\n",
    "    print(f\"\\nOriginal features shape: {X.shape}\")\n",
    "\n",
    "    # Apply feature transformations\n",
    "    X_transformed = transform_features(X)\n",
    "    print(f\"Transformed features shape: {X_transformed.shape}\")\n",
    "\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_transformed)\n",
    "\n",
    "    # If no specific n_components to test, we'll find optimal\n",
    "    if test_n_components is None:\n",
    "        # Test different numbers of components\n",
    "        test_n_components = [2, 5, 10, 15, 20, 30, 50, 75, 100]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for n_components in test_n_components:\n",
    "        print(f\"\\nTesting with {n_components} PCA components:\")\n",
    "\n",
    "        # Apply PCA\n",
    "        pca = MyPCA(n_components=n_components)\n",
    "        X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "        # Print explained variance\n",
    "        print(f\"Explained variance ratio: {pca.explained_variance_ratio}\")\n",
    "        print(f\"Cumulative explained variance: {pca.cumulative_explained_variance_ratio[-1]:.4f}\")\n",
    "\n",
    "        # Apply KMeans clustering\n",
    "        from sklearn.cluster import KMeans\n",
    "        kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "        clusters = kmeans.fit_predict(X_pca)\n",
    "\n",
    "        # Convert class labels to binary (0, 1)\n",
    "        # Assuming 'Cancer' should be 1 and 'Normal' is 0\n",
    "        y_true = (y == 'Cancer').astype(int)\n",
    "\n",
    "        # We need to check if cluster labels match actual labels or are flipped\n",
    "        # Calculate accuracy for original and flipped labels\n",
    "        acc_original = accuracy_score(y_true, clusters)\n",
    "        acc_flipped = accuracy_score(y_true, 1 - clusters)\n",
    "\n",
    "        # Use the mapping that gives higher accuracy\n",
    "        if acc_original >= acc_flipped:\n",
    "            y_pred = clusters\n",
    "            accuracy = acc_original\n",
    "        else:\n",
    "            y_pred = 1 - clusters\n",
    "            accuracy = acc_flipped\n",
    "\n",
    "        # Calculate metrics\n",
    "        precision = precision_score(y_true, y_pred)\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        silhouette = silhouette_score(X_pca, clusters)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        print(f\"Silhouette Score: {silhouette:.4f}\")\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            'n_components': n_components,\n",
    "            'explained_variance': pca.cumulative_explained_variance_ratio[-1],\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'silhouette': silhouette\n",
    "        })\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(cm)\n",
    "\n",
    "    # Convert results to dataframe\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Find best n_components based on accuracy\n",
    "    best_idx = results_df['accuracy'].idxmax()\n",
    "    best_n_components = results_df.loc[best_idx, 'n_components']\n",
    "    best_accuracy = results_df.loc[best_idx, 'accuracy']\n",
    "\n",
    "    print(f\"\\nBest n_components: {best_n_components} with accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(results_df['n_components'], results_df['explained_variance'], marker='o')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title('Explained Variance vs. Components')\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(results_df['n_components'], results_df['accuracy'], marker='o')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs. Components')\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(results_df['n_components'], results_df['f1'], marker='o')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('F1 Score vs. Components')\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(results_df['n_components'], results_df['silhouette'], marker='o')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title('Silhouette Score vs. Components')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pca_results.png')\n",
    "\n",
    "    # Now let's visualize the clustering with the best number of components\n",
    "    pca = MyPCA(n_components=best_n_components)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "    # For visualization, we'll use only the first 2 components\n",
    "    X_pca_2d = X_pca[:, :2]\n",
    "\n",
    "    # Apply KMeans\n",
    "    kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "    clusters = kmeans.fit_predict(X_pca)\n",
    "\n",
    "    # Check if we need to flip cluster labels\n",
    "    y_true = (y == 'Cancer').astype(int)\n",
    "    acc_original = accuracy_score(y_true, clusters)\n",
    "    acc_flipped = accuracy_score(y_true, 1 - clusters)\n",
    "\n",
    "    if acc_original >= acc_flipped:\n",
    "        y_pred = clusters\n",
    "    else:\n",
    "        y_pred = 1 - clusters\n",
    "\n",
    "    # Plot the clusters\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot based on predicted clusters\n",
    "    plt.subplot(1, 2, 1)\n",
    "    scatter = plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=y_pred, cmap='viridis', alpha=0.6)\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title('Clustering Results')\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "\n",
    "    # Plot based on true labels\n",
    "    plt.subplot(1, 2, 2)\n",
    "    scatter = plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=y_true, cmap='viridis', alpha=0.6)\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title('True Labels')\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('clustering_visualization.png')\n",
    "\n",
    "    return results_df"
   ],
   "id": "562af503ff760a23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Example usage\n",
    "# results = main(\"path_to_your_dataset.csv\")\n",
    "\n",
    "# For testing with specific number of components:\n",
    "results = main(\"../Dataset/ABIDE2.csv\", test_n_components=[10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150])"
   ],
   "id": "a2e58f0f534da510"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
